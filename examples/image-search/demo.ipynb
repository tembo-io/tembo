{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Modal Image Search on Postgres\n",
    "\n",
    "Use Postgres to search images using text or existing images.\n",
    "\n",
    "See `README.md` for setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import psycopg\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    CLIPImageProcessor,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    ")\n",
    "\n",
    "MODEL = \"openai/clip-vit-base-patch32\"\n",
    "DATABASE_URL = \"postgresql://postgres:postgres@localhost:5433/postgres\"\n",
    "IMAGE_DIR = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all the images\n",
    "\n",
    "We are using the [Amazon Products](https://www.kaggle.com/datasets/spypsc07/amazon-products?resource=download) dataset from Kaggle.\n",
    "Download that dataset if you have not already done so.\n",
    "\n",
    "Each product in the dataset has an image associated with it. We will download all the images to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/amazon_product.csv\")\n",
    "df.head(3)[[\"asin\", \"product_photo\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the dataframe of products and download each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    url = row[\"product_photo\"]\n",
    "    asin = row[\"asin\"]\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    if img.mode == 'RGBA':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    img.save(f\"./data/{asin}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate CLIP models\n",
    "\n",
    "We'll use APIs from the [transformers](https://huggingface.co/docs/transformers/en/model_doc/clip) library to generate embeddings from the images that we just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we're instantiating several objects, but all using the same `MODEL`\n",
    "image_processor = CLIPImageProcessor.from_pretrained(MODEL)\n",
    "image_model = CLIPModel.from_pretrained(MODEL)\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are a list of floats and we want to be sure we can keep them associated with the image they came from.\n",
    "So during processing, we'll create a `ImageEmbedding` object to hold the image path and the embeddings.\n",
    "\n",
    "The `ImageEmbedding` object is about the same as the schema for the table storing this data in Postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbedding(BaseModel):\n",
    "    image_path: str\n",
    "    embeddings: list[float]\n",
    "\n",
    "def pg_insert_embeddings(images: list[ImageEmbedding]):\n",
    "    # make sure pgvector is enabled\n",
    "    init_pg_vectorize = \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
    "\n",
    "    # create the table that will store embeddings for each image\n",
    "    init_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS image_embeddings (image_path TEXT PRIMARY KEY, embeddings VECTOR(512));\n",
    "    \"\"\"\n",
    "\n",
    "    # insert the embeddings for each image, updating if the image already exists\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO image_embeddings (image_path, embeddings)\n",
    "        VALUES (%s, %s)\n",
    "        ON CONFLICT (image_path)\n",
    "        DO UPDATE SET embeddings = EXCLUDED.embeddings\n",
    "        ;\n",
    "    \"\"\"\n",
    "    with psycopg.connect(DATABASE_URL) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(init_pg_vectorize)\n",
    "            cur.execute(init_table)\n",
    "\n",
    "            for image in images:\n",
    "                cur.execute(insert_query, (image.image_path, image.embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a couple helper functions to.\n",
    "\n",
    "One to find all the images that we just downloaded, and another to generate the embeddings for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_jpg_files(directory: str) -> list[str]:\n",
    "    # List to hold the full paths of files\n",
    "    full_paths = []\n",
    "    # Loop through the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file ends with .jpg\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            # Construct full path and add it to the list\n",
    "            full_paths.append(os.path.join(directory, filename))\n",
    "    return full_paths\n",
    "\n",
    "def get_image_embeddings(\n",
    "    image_paths: list[str], normalize=True\n",
    ") -> list[ImageEmbedding]:\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        images.append(Image.open(path))\n",
    "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = image_model.get_image_features(**inputs)\n",
    "\n",
    "    image_embeddings: list[ImageEmbedding] = []\n",
    "    for image_p, embedding in zip(image_paths, outputs):\n",
    "        if normalize:\n",
    "            embeds = F.normalize(embedding, p=2, dim=-1)\n",
    "        else:\n",
    "            embeds = embedding\n",
    "        image_embeddings.append(\n",
    "            ImageEmbedding(\n",
    "                image_path=image_p.split(\"/\")[-1].split(\".jpg\")[0],\n",
    "                embeddings=embeds.tolist(),\n",
    "            )\n",
    "        )\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get all the images, generate the embeddings, and write them into Postgres.\n",
    "\n",
    "If you haven't already done so, make sure Postgres is running `docker compose up postgres -d` will do. Or set the global `DATABASE_URL` variable to any Postgres with `pgvector` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list_jpg_files(IMAGE_DIR)\n",
    "image_embeddings = get_image_embeddings(images)\n",
    "pg_insert_embeddings(image_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Text to Embeddings\n",
    "\n",
    "In order to search the images using a text query, we need to be able to transform the text into embeddings as well.\n",
    "Let's set up a helper function to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(text, normalize=True) -> list[float]:\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    text_features = clip_model.get_text_features(**inputs)\n",
    "    text_embedding = text_features[0].detach().numpy()\n",
    "\n",
    "    if normalize:\n",
    "        embeds = text_embedding / np.linalg.norm(text_embedding)\n",
    "    else:\n",
    "        embeds = text_embedding\n",
    "    return embeds.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search the images in Postgres, we need to craft a query that will conduct a similarity search between the text embeddings and the image embeddings.\n",
    "\n",
    "[pgvector](https://github.com/pgvector/pgvector?tab=readme-ov-file#querying) supports a number of different distance operators, but we'll use the cosine similarity for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(txt_embedding: list[float]) -> list[tuple[str, float]]:\n",
    "    with psycopg.connect(DATABASE_URL) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                    SELECT\n",
    "                        image_path,\n",
    "                        1 - (embeddings <=> %s::vector) AS similarity_score\n",
    "                    FROM image_embeddings\n",
    "                    ORDER BY similarity_score DESC\n",
    "                    LIMIT 5;\n",
    "                \"\"\",\n",
    "                (txt_embedding,),\n",
    "            )\n",
    "            rows = cur.fetchall()\n",
    "\n",
    "            return [(row[0], row[1]) for row in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Images with a Text Query\n",
    "\n",
    "We have everything needed to search the images with a text query now.\n",
    "\n",
    "Piece the helper functions together to search the images with a text query and print the results out into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"arts and crafts\"\n",
    "\n",
    "text_embeddings = get_text_embeddings(search_query)\n",
    "results = similarity_search(text_embeddings)\n",
    "\n",
    "for r, score in results[:3]:\n",
    "    print(\"Image ID:\", r, \"Score:\", score)\n",
    "    image_path = IMAGE_DIR + \"/\" + r + \".jpg\"\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    img_resized = image.resize((300, 300))\n",
    "    display(img_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity Search\n",
    "\n",
    "We can also search these images by using another image as the search query.\n",
    "\n",
    "Download a Photo of Cher from Wikipedia and save it to the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Cher_in_2019_cropped_1.jpg/800px-Cher_in_2019_cropped_1.jpg\"\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img.save('./cher_wikipedia.jpg')\n",
    "\n",
    "\n",
    "# Rescale the image so it fits into this noitebook\n",
    "\n",
    "original_width, original_height = img.size\n",
    "new_width = int(original_width * 0.25)\n",
    "new_height = int(original_height * 0.25)\n",
    "\n",
    "resized_img = img.resize((new_width, new_height))\n",
    "display(resized_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find images similar to Cher's wikipedia image\n",
    "\n",
    "We saved the image of Cher to `./cher_wikipedia.jpg` above, so let's pass that into our embeddings function and use it to search for similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = get_image_embeddings([\"./cher_wikipedia.jpg\"])[0].embeddings\n",
    "results = similarity_search(image_embeddings)\n",
    "\n",
    "for r, score in results[:3]:\n",
    "    print(\"Image ID:\", r, \"Score:\", score)\n",
    "    image_path = IMAGE_DIR + \"/\" + r + \".jpg\"\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    img_resized = image.resize((250, 250))\n",
    "    display(img_resized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

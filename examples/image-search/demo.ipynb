{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPTokenizerFast,\n",
    "    CLIPTextModel,\n",
    "    CLIPImageProcessor,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    ")\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display\n",
    "import psycopg\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "MODEL = \"openai/clip-vit-base-patch32\"\n",
    "DATABASE_URL = \"postgresql://postgres:postgres@0.0.0.0:5433/postgres\"\n",
    "IMAGE_DIR = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/amazon_product.csv\")\n",
    "df.head(3)[[\"asin\", \"product_photo\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    url = row[\"product_photo\"]\n",
    "    asin = row[\"asin\"]\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    if img.mode == 'RGBA':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    img.save(f\"./data/{asin}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate CLIP models\n",
    "\n",
    "We'll use APIs from the `transformers` library to generate embeddings from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizerFast.from_pretrained(MODEL)\n",
    "text_model = CLIPTextModel.from_pretrained(MODEL)\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(MODEL)\n",
    "image_model = CLIPModel.from_pretrained(MODEL)\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageEmbedding(BaseModel):\n",
    "    image_id: str\n",
    "    embeddings: list[float]\n",
    "\n",
    "\n",
    "def get_image_embeddings(\n",
    "    image_paths: list[str], normalize=True\n",
    ") -> list[ImageEmbedding]:\n",
    "    # Process image and generate embeddings\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        images.append(Image.open(path))\n",
    "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = image_model.get_image_features(**inputs)\n",
    "\n",
    "    image_embeddings: list[ImageEmbedding] = []\n",
    "    for image_p, embedding in zip(image_paths, outputs):\n",
    "        if normalize:\n",
    "            embeds = F.normalize(embedding, p=2, dim=-1)\n",
    "        else:\n",
    "            embeds = embedding\n",
    "        image_embeddings.append(\n",
    "            ImageEmbedding(\n",
    "                image_id=image_p.split(\"/\")[-1].split(\".jpg\")[0],\n",
    "                embeddings=embeds.tolist(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return image_embeddings\n",
    "\n",
    "def list_jpg_files(directory):\n",
    "    # List to hold the full paths of files\n",
    "    full_paths = []\n",
    "    # Loop through the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file ends with .jpg\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            # Construct full path and add it to the list\n",
    "            full_paths.append(os.path.join(directory, filename))\n",
    "    return full_paths\n",
    "\n",
    "\n",
    "def pg_insert_embeddings(images: list[ImageEmbedding]):\n",
    "    init_pg_vectorize = \"CREATE EXTENSION IF NOT EXISTS vectorize CASCADE;\"\n",
    "    init_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS image_embeddings (image_id TEXT PRIMARY KEY, embeddings VECTOR(512));\n",
    "    \"\"\"\n",
    "    insert_query = \"\"\"\n",
    "        INSERT INTO image_embeddings (image_id, embeddings)\n",
    "        VALUES (%s, %s)\n",
    "        ON CONFLICT (image_id)\n",
    "        DO UPDATE SET embeddings = EXCLUDED.embeddings\n",
    "        ;\n",
    "    \"\"\"\n",
    "    with psycopg.connect(DATABASE_URL) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(init_pg_vectorize)\n",
    "            cur.execute(init_table)\n",
    "\n",
    "            for image in images:\n",
    "                cur.execute(insert_query, (image.image_id, image.embeddings))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list_jpg_files(IMAGE_DIR)\n",
    "image_embeddings = get_image_embeddings(images)\n",
    "pg_insert_embeddings(image_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Text to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(text, normalize=True) -> list[float]:\n",
    "    inputs = processor(text=[text], return_tensors=\"pt\", padding=True)\n",
    "    text_features = clip_model.get_text_features(**inputs)\n",
    "    text_embedding = text_features[0].detach().numpy()\n",
    "\n",
    "    if normalize:\n",
    "        embeds = text_embedding / np.linalg.norm(text_embedding)\n",
    "    else:\n",
    "        embeds = text_embedding\n",
    "    return embeds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(txt_embedding: list[float]) -> list[tuple[str, float]]:\n",
    "    with psycopg.connect(DATABASE_URL) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                    SELECT\n",
    "                        image_id,\n",
    "                        1 - (embeddings <=> %s::vector) AS similarity_score\n",
    "                    FROM image_embeddings\n",
    "                    ORDER BY similarity_score DESC\n",
    "                    LIMIT 5;\n",
    "                \"\"\",\n",
    "                (txt_embedding,),\n",
    "            )\n",
    "            rows = cur.fetchall()\n",
    "\n",
    "            return [(row[0], row[1]) for row in rows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Images with a Text Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = get_text_embeddings(\"arts and crafts\")\n",
    "results = similarity_search(text_embeddings)\n",
    "\n",
    "for r, score in results[:3]:\n",
    "    print(\"Image ID:\", r, \"Score:\", score)\n",
    "    image_path = IMAGE_DIR + \"/\" + r + \".jpg\"\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    img_resized = image.resize((300, 300))\n",
    "    display(img_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity Search\n",
    "\n",
    "Download a Photo of Cher from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image of Cher from wikipedia\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Cher_in_2019_cropped_1.jpg/752px-Cher_in_2019_cropped_1.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img.save('./cher_wikipedia.jpg')\n",
    "original_width, original_height = img.size\n",
    "new_width = int(original_width * 0.25)\n",
    "new_height = int(original_height * 0.25)\n",
    "\n",
    "# Rescale the image\n",
    "resized_img = img.resize((new_width, new_height))\n",
    "display(resized_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find images similar to Cher's wikipedia image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = get_image_embeddings([\"./cher_wikipedia.jpg\"])[0].embeddings\n",
    "results = similarity_search(image_embeddings)\n",
    "\n",
    "for r, score in results[:3]:\n",
    "    print(\"Image ID:\", r, \"Score:\", score)\n",
    "    image_path = IMAGE_DIR + \"/\" + r + \".jpg\"\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    img_resized = image.resize((250, 250))\n",
    "    display(img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
